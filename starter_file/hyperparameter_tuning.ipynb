{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using HyperDrive\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "gather": {
     "logged": 1704009512349
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "import requests\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core import Workspace, Dataset, Experiment\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azure.ai.ml.entities import Environment, Model\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.core.compute import AmlCompute ,ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azure.ai.ml.sweep import BanditPolicy\n",
    "from azureml.train.hyperdrive.parameter_expressions import uniform, choice\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azure.ai.ml import MLClient, command\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "gather": {
     "logged": 1704009516021
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ./config.json\n"
     ]
    }
   ],
   "source": [
    "#Create MLClient object, this object is like a Workspace handle\n",
    "\n",
    "ML_CLIENT = MLClient.from_config(DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external.\n",
    "\n",
    "**The dataset is accutally needed in the train.py script, so please look at that file to check the code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1704009520404
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "experiment_name = 'hyperdrive-exper'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1704009526822
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "\n",
      "Running\n"
     ]
    }
   ],
   "source": [
    "amlcompute_cluster_name = \"notebook247476\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',                                                           \n",
    "                                                           max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1598531923519
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Hyperdrive Configuration\n",
    "\n",
    "TODO: Explain the model you are using and the reason for chosing the different hyperparameters, termination policy and config settings.\n",
    "I'm using the LinearSVM model, because it suitable with the dataset(classification problem, data is numeric).\n",
    "The 2 params used are.\n",
    "+ C\n",
    "+ max_iter\n",
    "\n",
    "Because these are two of very importance params of the SVM model. To work futher, I might expand the search space with more parameter and wide range value.\n",
    "\n",
    "The termination policy is BanditPolicy with `BanditPolicy(slack_factor=0.15, evaluation_interval=1, delay_evaluation=10)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "gather": {
     "logged": 1704019023427
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import command, Input\n",
    "from azure.ai.ml.sweep import Choice, Uniform, MedianStoppingPolicy\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_factor=0.15, evaluation_interval=1, delay_evaluation=10)\n",
    "command_job_for_sweep = command(\n",
    "    code=\"./\",\n",
    "    command=\"python train.py --dataset_name ${{inputs.dataset_name}} --C ${{inputs.C}} --max_iter ${{inputs.max_iter}}\",\n",
    "    environment=Environment(image=\"mcr.microsoft.com/azureml/curated/responsibleai-ubuntu20.04-py38-cpu:38\"),\n",
    "    inputs={\n",
    "        \"dataset_name\": \"IRIS-Dataset\",\n",
    "        #define the search space for hyperparameters\n",
    "        \"C\": Uniform(min_value=0.01, max_value=1),\n",
    "        \"max_iter\": Choice(values=[50, 100, 200]),\n",
    "    },\n",
    "    compute=\"cpu-cluster\",\n",
    ")\n",
    "\n",
    "sweep_job = command_job_for_sweep.sweep(\n",
    "    compute=amlcompute_cluster_name, \n",
    "    sampling_algorithm=\"random\",\n",
    "    primary_metric=\"Accuracy\",\n",
    "    goal=\"Maximize\",\n",
    "    early_termination_policy=early_termination_policy\n",
    ")\n",
    "\n",
    "sweep_job.set_limits(max_total_trials=20, max_concurrent_trials=10, timeout=7200)\n",
    "# Specify your experiment details\n",
    "sweep_job.display_name = \"hp-iris\"\n",
    "sweep_job.experiment_name = \"hp-iris\"\n",
    "sweep_job.description = \"Run a hyperparameter sweep job on Iris dataset.\"\n",
    "\n",
    "\n",
    "# submit the sweep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1704019023455
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading odl_user_247476 (0.5 MBs): 100%|██████████| 497304/497304 [00:00<00:00, 3416504.22it/s]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: Submit your experiment\n",
    "returned_sweep_job = ML_CLIENT.create_or_update(sweep_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "gather": {
     "logged": 1704019023486
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: olive_library_qpm1j0g79c\n",
      "Web View: https://ml.azure.com/runs/olive_library_qpm1j0g79c?wsid=/subscriptions/cdbe0b43-92a0-4715-838a-f2648cc7ad21/resourcegroups/aml-quickstarts-247476/workspaces/quick-starts-ws-247476\n",
      "\n",
      "Streaming azureml-logs/hyperdrive.txt\n",
      "=====================================\n",
      "\n",
      "[2023-12-31T11:00:20.076742][GENERATOR][INFO]Trying to sample '10' jobs from the hyperparameter space\n",
      "[2023-12-31T11:00:20.5389318Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_0' \n",
      "[2023-12-31T11:00:20.7782580Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_2' \n",
      "[2023-12-31T11:00:20.7770264Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_1' \n",
      "[2023-12-31T11:00:20.8918988Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_3' \n",
      "[2023-12-31T11:00:21.0252133Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_4' \n",
      "[2023-12-31T11:00:21.1242149Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_5' \n",
      "[2023-12-31T11:00:21.2348949Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_1' \n",
      "[2023-12-31T11:00:21.2285377Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_0' \n",
      "[2023-12-31T11:00:21.2320261Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_6' \n",
      "[2023-12-31T11:00:21.2783813Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_4' \n",
      "[2023-12-31T11:00:21.3610273Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_2' \n",
      "[2023-12-31T11:00:21.3105505Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_3' \n",
      "[2023-12-31T11:00:21.3578885Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_5' \n",
      "[2023-12-31T11:00:21.3762870Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_7' \n",
      "[2023-12-31T11:00:21.4513045Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_6' \n",
      "[2023-12-31T11:00:21.5512967Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_8' \n",
      "[2023-12-31T11:00:21.6317077Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_9' \n",
      "[2023-12-31T11:00:21.6347251Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_7' \n",
      "[2023-12-31T11:00:21.569137][GENERATOR][INFO]Successfully sampled '10' jobs, they will soon be submitted to the execution target.\n",
      "[2023-12-31T11:00:21.7896448Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_8' \n",
      "[2023-12-31T11:00:21.8555317Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_9' \n",
      "[2023-12-31T11:02:21.243219][GENERATOR][INFO]Trying to sample '4' jobs from the hyperparameter space\n",
      "[2023-12-31T11:02:21.5711258Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_10' \n",
      "[2023-12-31T11:02:21.7004697Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_11' \n",
      "[2023-12-31T11:02:21.8350037Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_10' \n",
      "[2023-12-31T11:02:21.8654838Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_12' \n",
      "[2023-12-31T11:02:21.9856824Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_13' \n",
      "[2023-12-31T11:02:22.0900357Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_11' \n",
      "[2023-12-31T11:02:21.922820][GENERATOR][INFO]Successfully sampled '4' jobs, they will soon be submitted to the execution target.\n",
      "[2023-12-31T11:02:22.0843173Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_12' \n",
      "[2023-12-31T11:02:22.3108976Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_13' \n",
      "[2023-12-31T11:03:21.220888][GENERATOR][INFO]Trying to sample '4' jobs from the hyperparameter space\n",
      "[2023-12-31T11:03:21.5419103Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_14' \n",
      "[2023-12-31T11:03:21.7597323Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_14' \n",
      "[2023-12-31T11:03:21.7452168Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_15' \n",
      "[2023-12-31T11:03:21.8725261Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_16' \n",
      "[2023-12-31T11:03:21.9422362Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_17' \n",
      "[2023-12-31T11:03:21.9788044Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_15' \n",
      "[2023-12-31T11:03:21.853044][GENERATOR][INFO]Successfully sampled '4' jobs, they will soon be submitted to the execution target.\n",
      "[2023-12-31T11:03:22.1240618Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_16' \n",
      "[2023-12-31T11:03:22.2345548Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_17' \n",
      "[2023-12-31T11:03:51.235339][GENERATOR][INFO]Trying to sample '2' jobs from the hyperparameter space\n",
      "[2023-12-31T11:03:51.6597540Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_18' \n",
      "[2023-12-31T11:03:51.7893733Z][SCHEDULER][INFO]Scheduling job, id='olive_library_qpm1j0g79c_19' \n",
      "[2023-12-31T11:03:51.696173][GENERATOR][INFO]Successfully sampled '2' jobs, they will soon be submitted to the execution target.\n",
      "[2023-12-31T11:03:51.9062508Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_18' \n",
      "[2023-12-31T11:03:52.0369392Z][SCHEDULER][INFO]Successfully scheduled a job. Id='olive_library_qpm1j0g79c_19' \n",
      "[2023-12-31T11:04:21.233757][GENERATOR][INFO]Max number of jobs '20' reached for experiment.\n",
      "[2023-12-31T11:04:21.368399][GENERATOR][INFO]All jobs generated.\n",
      "[2023-12-31T11:05:22.4685058Z][CONTROLLER][INFO]Changing Run Status from Running to Completed \n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: olive_library_qpm1j0g79c\n",
      "Web View: https://ml.azure.com/runs/olive_library_qpm1j0g79c?wsid=/subscriptions/cdbe0b43-92a0-4715-838a-f2648cc7ad21/resourcegroups/aml-quickstarts-247476/workspaces/quick-starts-ws-247476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ML_CLIENT.jobs.stream(returned_sweep_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "gather": {
     "logged": 1598544898497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments.\n",
    "\n",
    "**I'm using SDK v2 sweepjob to run which doesn't support RunDetails.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the hyperdrive experiments and display all the properties of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment. Model is deploy successfully. Look at picture below (^<>^)\n",
    "//TODO\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained but you still need to register both the models. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best model is get here. The code below download the best run model to local files. Then, these  files will be use to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "gather": {
     "logged": 1704019023197
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifact azureml://subscriptions/cdbe0b43-92a0-4715-838a-f2648cc7ad21/resourcegroups/aml-quickstarts-247476/workspaces/quick-starts-ws-247476/datastores/workspaceartifactstore/paths/ExperimentRun/dcid.olive_library_qpm1j0g79c_6/model to hp-deploy/named-outputs/mlflow_log_model_1902316930\n",
      "Downloading artifact azureml://subscriptions/cdbe0b43-92a0-4715-838a-f2648cc7ad21/resourcegroups/aml-quickstarts-247476/workspaces/quick-starts-ws-247476/datastores/workspaceartifactstore/paths/ExperimentRun/dcid.olive_library_qpm1j0g79c_6/hp to hp-deploy/named-outputs/mlflow_log_model_806180258\n",
      "Downloading artifact azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.olive_library_qpm1j0g79c_6 to hp-deploy/artifacts\n",
      "Downloading artifact azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.olive_library_qpm1j0g79c_6 to hp-deploy/hd-artifacts\n"
     ]
    }
   ],
   "source": [
    "# Download best model to local\n",
    "DEPLOY_PATH = \"hp-deploy\"\n",
    "os.makedirs(DEPLOY_PATH, exist_ok=True)\n",
    "MODEL_PATH = MODEL_NAME = \"model\"\n",
    "ML_CLIENT.jobs.download(name=returned_sweep_job.name, download_path=DEPLOY_PATH, all = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And the best model screenshot is HERE :))\n",
    "// TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach the endpoint and model to workspace\n",
    "    * Register the endpoint\n",
    "    * Register the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "gather": {
     "logged": 1704017005013
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineEndpoint\n",
    "import datetime\n",
    "endpoint_name = f\"hp-{datetime.datetime.now().strftime('%m%d%H%M%f')}\"\n",
    "ENDPOINT = ManagedOnlineEndpoint(\n",
    "    name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "gather": {
     "logged": 1704017005030
    }
   },
   "outputs": [],
   "source": [
    "# Register the end point\n",
    "endpoint = ML_CLIENT.begin_create_or_update(ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "gather": {
     "logged": 1704017005046
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting endpoint deployed...\n",
      "Deployed endpoint hp-12311105866892 success. Status: Succeeded\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time.sleep(2)\n",
    "print(\"Waiting endpoint deployed...\")\n",
    "status = ML_CLIENT.online_endpoints.get(endpoint_name).provisioning_state\n",
    "while status == 'Creating':\n",
    "    time.sleep(1)\n",
    "    status = ML_CLIENT.online_endpoints.get(endpoint_name).provisioning_state\n",
    "\n",
    "if (status == \"Succeeded\"):\n",
    "    print(f\"Deployed endpoint {endpoint_name} success. Status: {status}\")\n",
    "else:\n",
    "    print(f\"Deployed endpoint {endpoint_name} failed. Status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1704011517052
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading model (0.0 MBs): 100%|██████████| 2183/2183 [00:00<00:00, 60471.06it/s]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model({'job_name': None, 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'hp-model-1', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': '/subscriptions/cdbe0b43-92a0-4715-838a-f2648cc7ad21/resourceGroups/aml-quickstarts-247476/providers/Microsoft.MachineLearningServices/workspaces/quick-starts-ws-247476/models/hp-model-1/versions/1', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/notebook247476/code/Users/odl_user_247476', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f99a4212b50>, 'serialize': <msrest.serialization.Serializer object at 0x7f99a4212610>, 'version': '1', 'latest_version': None, 'path': 'azureml://subscriptions/cdbe0b43-92a0-4715-838a-f2648cc7ad21/resourceGroups/aml-quickstarts-247476/workspaces/quick-starts-ws-247476/datastores/workspaceblobstore/paths/LocalUpload/93f6bbbce0ff7f96922faa9288585a34/model', 'datastore': None, 'utc_time_created': None, 'flavors': {'python_function': {'env': '{\\n  \"conda\": \"conda.yaml\",\\n  \"virtualenv\": \"python_env.yaml\"\\n}', 'loader_module': 'mlflow.sklearn', 'model_path': 'model.pkl', 'predict_fn': 'predict', 'python_version': '3.8.18'}, 'sklearn': {'code': '', 'pickled_model': 'model.pkl', 'serialization_format': 'cloudpickle', 'sklearn_version': '1.2.2'}}, 'arm_type': 'model_version', 'type': 'mlflow_model', 'stage': 'Development'})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = Model(\n",
    "        path=f\"{DEPLOY_PATH}/artifacts/{MODEL_PATH}\",\n",
    "        name=\"hp-model-2\",\n",
    "        type=AssetTypes.MLFLOW_MODEL\n",
    "    )\n",
    "ML_CLIENT.models.create_or_update(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Deployment object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1704011632789
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint hp-12311105866892 exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x7f998c9dcd00>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..."
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineDeployment, CodeConfiguration\n",
    "deployment_name = \"snow-wolf\"\n",
    "DEPLOYMENT = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=model,\n",
    "    instance_type=\"Standard_DS3_v2\",\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "ML_CLIENT.begin_create_or_update(DEPLOYMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"log.log\", \"w\") as f:\n",
    "\n",
    "    f.write(ML_CLIENT.online_deployments.get_logs(deployment_name, endpoint_name, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1704012103567
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting deployment deploying...\n",
      "..............................................................................................Deployed deployment snow-wolf success. Status: Succeeded\n",
      "."
     ]
    }
   ],
   "source": [
    "import time\n",
    "print(\"Waiting deployment deploying...\")\n",
    "status = ML_CLIENT.online_deployments.get(deployment_name, endpoint_name).provisioning_state\n",
    "while status == 'Updating':\n",
    "    time.sleep(1)\n",
    "    status = ML_CLIENT.online_deployments.get(deployment_name, endpoint_name).provisioning_state\n",
    "\n",
    "if (status == \"Succeeded\"):\n",
    "    print(f\"Deployed deployment {deployment_name} success. Status: {status}\")\n",
    "else:\n",
    "    print(f\"Deployed deployment {deployment_name} failed. Status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x7ff86498b490>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".."
     ]
    }
   ],
   "source": [
    "ML_CLIENT.online_deployments.begin_delete(deployment_name, endpoint_name)\n",
    "ML_CLIENT.online_endpoints.begin_delete(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[2]'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "# Request data goes here\n",
    "# The example below assumes JSON formatting which may be updated\n",
    "# depending on the format your endpoint expects.\n",
    "# More information can be found here:\n",
    "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
    "data =  {\n",
    "  \"input_data\": {\n",
    "    \"columns\": [\n",
    "      \"SepalLengthCm\",\n",
    "      \"SepalWidthCm\",\n",
    "      \"PetalLengthCm\",\n",
    "      \"PetalWidthCm\"\n",
    "    ],\n",
    "    \"index\": [1],\n",
    "    \"data\": [[2, 3, 4, 5]]\n",
    "  },\n",
    "  \"params\": {}\n",
    "}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = ML_CLIENT.online_endpoints.get(endpoint_name).scoring_uri\n",
    "api_key = ML_CLIENT.online_endpoints.get_keys(endpoint_name)\n",
    "\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key.primary_key), 'azureml-model-deployment': 'snow-wolf' }\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get log\n",
    "ML_CLIENT.online_deployments.get_logs(name=\"AutoMlDeployment\", endpoint_name=endpoint_name, lines=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the service\n",
    "ML_CLIENT.online_endpoints.begin_delete(name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission Checklist**\n",
    "- I have registered the model.\n",
    "- I have deployed the model with the best accuracy as a webservice.\n",
    "- I have tested the webservice by sending a request to the model endpoint.\n",
    "- I have deleted the webservice and shutdown all the computes that I have used.\n",
    "- I have taken a screenshot showing the model endpoint as active.\n",
    "- The project includes a file containing the environment details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
